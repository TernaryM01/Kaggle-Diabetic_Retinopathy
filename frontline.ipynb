{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f3ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "trainval_labels1 = pd.read_csv('aptos2019-blindness-detection/train.csv')\n",
    "trainval_imgs_dir1 = Path('aptos2019-blindness-detection/train_images/processed')\n",
    "\n",
    "trainval_labels2 = pd.read_csv('diabetic-retinopathy-detection/trainLabels.csv')\n",
    "trainval_imgs_dir2 = Path('diabetic-retinopathy-detection/train/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042137db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize trainval_labels1\n",
    "df1 = trainval_labels1.copy()\n",
    "df1.columns = ['id', 'label']\n",
    "df1['group'] = df1['id']\n",
    "df1['img_dir'] = df1['id'].apply(lambda x: trainval_imgs_dir1 / f\"{x}.png\")\n",
    "\n",
    "# Standardize trainval_labels2\n",
    "df2 = trainval_labels2.copy()\n",
    "df2.columns = ['id', 'label']\n",
    "df2['group'] = df2['id'].apply(lambda x: x.split('_')[0])\n",
    "df2['img_dir'] = df2['id'].apply(lambda x: trainval_imgs_dir2 / f\"{x}.jpeg\")\n",
    "\n",
    "# Combine both\n",
    "df_trainval = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3b6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Initialize splitter for an 80/20 split\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform the split\n",
    "train_idx, val_idx = next(splitter.split(df_trainval, groups=df_trainval['group']))\n",
    "\n",
    "# Use the indices to create train and val DataFrames\n",
    "df_train = df_trainval.iloc[train_idx].copy().drop(columns='group')\n",
    "df_val = df_trainval.iloc[val_idx].copy().drop(columns='group')\n",
    "\n",
    "df_train.to_csv('df_train.csv', index=False)\n",
    "df_val.to_csv('df_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49c886",
   "metadata": {},
   "source": [
    "=============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4299aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_val = pd.read_csv('df_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50961e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # id = row['id']\n",
    "        label = int(row['label'])\n",
    "        img_dir = row['img_dir']\n",
    "\n",
    "        image = Image.open(img_dir).convert('RGB')  # ensure 3 channels\n",
    "\n",
    "        if self.transform:\n",
    "            image = np.array(image)\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eed412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/albumentations/check_version.py:107: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "IMAGE_SIZE_TRAIN = 352\n",
    "IMAGE_SIZE_VAL = 480\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE_VAL, IMAGE_SIZE_VAL),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=360, p=1.0),\n",
    "    A.RandomCrop(IMAGE_SIZE_TRAIN, IMAGE_SIZE_TRAIN),\n",
    "\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=0.2,  # ±20% brightness\n",
    "        contrast_limit=0.2,    # ±20% contrast\n",
    "        p=0.3\n",
    "    ),\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=10,        # ±10 degrees\n",
    "        sat_shift_limit=20,        # ±20%\n",
    "        val_shift_limit=10,        # ±10%\n",
    "        p=0.3\n",
    "    ),\n",
    "    A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n",
    "    A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.3),\n",
    "\n",
    "    A.Normalize(  # For model pretrained on ImageNet\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE_VAL, IMAGE_SIZE_VAL),\n",
    "    A.Normalize(  # For model pretrained on ImageNet\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47540e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE_TRAIN = 40\n",
    "# BATCH_SIZE_VAL = 64\n",
    "BATCH_SIZE_VAL = 16\n",
    "\n",
    "train_dataset = TrainDataset(df_train, train_transform)\n",
    "val_dataset = TrainDataset(df_val, val_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True, num_workers=10)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437cdd6e",
   "metadata": {},
   "source": [
    "============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70de8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d83f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EfficientNetV2OrdinalClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"efficientnetv2_rw_m\", lr=1e-4, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Use timm to load pretrained backbone, remove classifier head\n",
    "        self.net = timm.create_model(\n",
    "            self.hparams.model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # remove original head\n",
    "            \n",
    "            drop_rate=0.3,\n",
    "            drop_path_rate=0.3\n",
    "        )\n",
    "\n",
    "        in_features = self.net.num_features\n",
    "        self.head = nn.Linear(in_features, self.num_classes - 1)  # 4 outputs for 5 ordinal classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.net(x)\n",
    "        logits = self.head(features)\n",
    "        return logits\n",
    "    \n",
    "    def predict_class(self, logits):\n",
    "        probas = logits.sigmoid()\n",
    "        return (probas > 0.5).sum(dim=1)\n",
    "    \n",
    "    def ordinal_targets(self, labels):\n",
    "        \"\"\"\n",
    "        Converts integer class labels (0 to num_classes - 1) into ordinal binary targets.\n",
    "        For example, label 2 becomes [1, 1, 0, 0] for num_classes = 5\n",
    "        \"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        num_thresholds = self.num_classes - 1  # one less than number of classes\n",
    "        labels_expanded = labels.unsqueeze(1)  # Expand labels to shape (batch_size, 1)\n",
    "        # Create comparison thresholds: shape (1, num_thresholds) = [0, 1, 2, 3]\n",
    "        thresholds = torch.arange(num_thresholds, device=labels.device).unsqueeze(0)\n",
    "        # Compare each label to thresholds: True where label > threshold\n",
    "        binary_targets = labels_expanded > thresholds  # shape (batch_size, num_thresholds)\n",
    "        return binary_targets.float()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        targets = self.ordinal_targets(labels)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "        \n",
    "        preds = (logits.sigmoid() > 0.5).sum(dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train_acc',  acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        targets = self.ordinal_targets(labels)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "\n",
    "        preds = (logits.sigmoid() > 0.5).sum(dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc',  acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=5e-5)\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',              # we're watching val_loss (lower is better)\n",
    "                factor=0.5,              # reduce LR by this factor\n",
    "                patience=5,              # after N epochs of no improvement\n",
    "                min_lr=1e-6,             # don’t go below this\n",
    "                verbose=True\n",
    "            ),\n",
    "            'monitor': 'val_loss',\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaabcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pytorch\n",
    "\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='effnet-v2rw-m-ordinal-dropout2-l2reg2-augs-full-{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=[earlystop_cb, checkpoint_cb],\n",
    "    accelerator='auto',  # GPU if available\n",
    "    precision='16-mixed',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "model = EfficientNetV2OrdinalClassifier(lr=1e-4, num_classes=5)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "print(\"✅ Best checkpoint:\", checkpoint_cb.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad77ca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a219036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AdjacentLabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, smoothing: float = 0.1, num_classes: int = 5, window_size: int = 1):\n",
    "        \"\"\"\n",
    "        Exponential-decay label smoothing for ordinal targets.\n",
    "\n",
    "        Args:\n",
    "          smoothing: total probability mass to smooth away from the true class (0 <= s < 1).\n",
    "          num_classes: total number of ordinal classes.\n",
    "          window_size: how many steps to consider on each side of the true class.\n",
    "                       (1 for adjacent only, 2 to include distance-2 neighbors, etc.)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert 0 <= smoothing < 1, \"smoothing must be in [0,1).\"\n",
    "        self.smoothing = smoothing\n",
    "        self.num_classes = num_classes\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Precompute smoothing distributions for each possible true class 0..num_classes-1\n",
    "        # according to your two requirements:\n",
    "        #   1) exp decay by distance\n",
    "        #   2) center weight = 1 - smoothing\n",
    "        R = smoothing / (1.0 - smoothing)  # ratio of total neighbor mass to center mass\n",
    "\n",
    "        weight_matrix = []\n",
    "        for t in range(num_classes):\n",
    "            # how many valid steps on each side\n",
    "            left_n  = min(window_size,             t)\n",
    "            right_n = min(window_size, num_classes - 1 - t)\n",
    "\n",
    "            # Solve for decay d so that sum(d^1..d^left_n) + sum(d^1..d^right_n) = R\n",
    "            # Newton's method on f(d) = sum_{k=1..L} d^k + sum_{k=1..R} d^k - R = 0\n",
    "            if left_n + right_n == 0:\n",
    "                d = 0.0\n",
    "            else:\n",
    "                # initial guess\n",
    "                d = R / (left_n + right_n)\n",
    "                for _ in range(50):\n",
    "                    # f(d) and f'(d)\n",
    "                    f = sum(d**k for k in range(1, left_n+1)) + sum(d**k for k in range(1, right_n+1)) - R\n",
    "                    fp = sum(k * d**(k-1) for k in range(1, left_n+1)) + sum(k * d**(k-1) for k in range(1, right_n+1))\n",
    "                    d = max(d - f/(fp + 1e-12), 1e-12)\n",
    "\n",
    "            # Build raw (unnormalized) weights for this true class t\n",
    "            raw = torch.zeros(num_classes, dtype=torch.float64)\n",
    "            for c in range(num_classes):\n",
    "                dist = abs(c - t)\n",
    "                if dist == 0:\n",
    "                    raw[c] = 1.0\n",
    "                elif dist <= window_size:\n",
    "                    raw[c] = d**dist\n",
    "                # else remains 0\n",
    "\n",
    "            # Normalize so sum(raw) = 1 and center weight = 1 - smoothing\n",
    "            raw = raw / raw.sum()\n",
    "            weight_matrix.append(raw.float())\n",
    "\n",
    "        # Stack into [num_classes, num_classes] tensor\n",
    "        weight_matrix = torch.stack(weight_matrix, dim=0)  # weight_matrix[t] is the distribution for true class t\n",
    "        self.register_buffer('weight_matrix', weight_matrix)\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        logits: (B, C)\n",
    "        target: (B,) integer tensor in [0..C-1]\n",
    "        \"\"\"\n",
    "        # Get the precomputed soft-target distributions\n",
    "        # shape → (B, C)\n",
    "        true_dist = self.weight_matrix[target]\n",
    "\n",
    "        # Standard cross-entropy with log-softmax\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        loss = -(true_dist * log_probs).sum(dim=-1).mean()\n",
    "        return loss\n",
    "    \n",
    "    def debug_dist(self, true_class: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the smoothed distribution for a given true class.\n",
    "\n",
    "        Args:\n",
    "            true_class: int, between 0 and num_classes - 1.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (num_classes,) showing the target distribution.\n",
    "        \"\"\"\n",
    "        if not (0 <= true_class < self.num_classes):\n",
    "            raise ValueError(f\"true_class must be between 0 and {self.num_classes - 1}, got {true_class}.\")\n",
    "        \n",
    "        dist = self.weight_matrix[true_class]\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfca7411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9000, 0.1000, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0500, 0.9000, 0.0500, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.0500, 0.9000, 0.0500, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "loss_fn = AdjacentLabelSmoothingLoss(smoothing=0.1, window_size=1)\n",
    "print(loss_fn.debug_dist(0))\n",
    "print(loss_fn.debug_dist(1))\n",
    "print(loss_fn.debug_dist(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d131cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"convnext_small.fb_in22k_ft_in1k_384\", lr=1e-4, num_classes=5, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # create & swap in a new head\n",
    "        self.net = timm.create_model(\n",
    "            self.hparams.model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=self.hparams.num_classes,\n",
    "            \n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2,\n",
    "        )\n",
    "        \n",
    "        self.criterion = AdjacentLabelSmoothingLoss(\n",
    "            smoothing=self.hparams.smoothing,\n",
    "            num_classes=self.hparams.num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def predict_class(self, logits):\n",
    "        return logits.argmax(dim=-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        acc  = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train_acc',  acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        acc  = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc',  acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=1e-5)\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',              # we're watching val_loss (lower is better)\n",
    "                factor=0.5,              # reduce LR by this factor\n",
    "                patience=5,              # after N epochs of no improvement\n",
    "                min_lr=1e-6,             # don’t go below this\n",
    "                verbose=True\n",
    "            ),\n",
    "            'monitor': 'val_loss',\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d453fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2Classifier(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"efficientnetv2_rw_m\", lr=1e-4, num_classes=5, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # create & swap in a new head\n",
    "        self.net = timm.create_model(\n",
    "            self.hparams.model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=self.hparams.num_classes,\n",
    "            \n",
    "            drop_rate=0.4,        # 🔥 add stronger dropout (applied before final FC)\n",
    "            drop_path_rate=0.3,   # 🔥 stochastic depth (helps regularize deep nets)\n",
    "        )\n",
    "        \n",
    "        self.criterion = AdjacentLabelSmoothingLoss(\n",
    "            smoothing=self.hparams.smoothing,\n",
    "            num_classes=self.hparams.num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def predict_class(self, logits):\n",
    "        return logits.argmax(dim=-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        acc  = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train_acc',  acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        acc  = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc',  acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=1e-4)\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',              # we're watching val_loss (lower is better)\n",
    "                factor=0.5,              # reduce LR by this factor\n",
    "                patience=5,              # after N epochs of no improvement\n",
    "                min_lr=1e-6,             # don’t go below this\n",
    "                verbose=True\n",
    "            ),\n",
    "            'monitor': 'val_loss',\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdd9d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Classifier(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"resnet50.a1_in1k\", lr=1e-4, num_classes=5,\n",
    "                 smoothing=0.1, window_size=1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # create & swap in a new head\n",
    "        self.net = timm.create_model(\n",
    "            self.hparams.model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=self.hparams.num_classes,\n",
    "            \n",
    "            drop_rate=0.3,        # 🔥 add stronger dropout (applied before final FC)\n",
    "            # drop_path_rate=0.3,   # 🔥 stochastic depth (helps regularize deep nets)\n",
    "        )\n",
    "        \n",
    "        self.criterion = AdjacentLabelSmoothingLoss(\n",
    "            smoothing=self.hparams.smoothing,\n",
    "            window_size=self.hparams.window_size,\n",
    "            num_classes=self.hparams.num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def predict_class(self, logits):\n",
    "        return logits.argmax(dim=-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc  = (preds == labels).float().mean()\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train_acc',  acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc  = (preds == labels).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_acc',  acc, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=1e-4)\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',              # we're watching val_loss (lower is better)\n",
    "                factor=0.5,              # reduce LR by this factor\n",
    "                patience=5,              # after N epochs of no improvement\n",
    "                min_lr=1e-6,             # don’t go below this\n",
    "                verbose=True\n",
    "            ),\n",
    "            'monitor': 'val_loss',\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc35716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "2025/04/28 13:05:58 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '55895ce18e1147208e09dbd5f0f57115', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pytorch workflow\n",
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /mnt/g/Kaggle-Diabetic-Retinopathy/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                       | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | net       | ResNet                     | 23.5 M | train\n",
      "1 | criterion | AdjacentLabelSmoothingLoss | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.073    Total estimated model params size (MB)\n",
      "218       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d76c61726d146a39d8d2843b5d4e237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655905a9a3c04f428359850f2e0147ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8a8082e76b4a3faee4099d4f2fcd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6881752b51a435793b6dfd92e4c4251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2872461bc1848daabde73a592436479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1deddcc43814408ea94ef31c3487e514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42fdb48cbb840648443fba929619b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be2186fd26e490bb3c48b4f6330295c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d5ff094aad4a9fb9870785aa460cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb42e2511d7e4247b158b889b1446d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9080e39d5dd34249a18aea7a28ba3dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a325af782d554c0ab89c4e4e9544cbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0996d8768d439c8b363f116f9eb45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0884ecdcb0d44afaa14df876f2814001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48583a556c9430e9f7c8ca7ab762b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0796f732db4c269ee20fc886de20fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08215ce5cad844d9a80bc496b290cabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0452e923a00240bd95603446ef10b1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01843b6f256341a4b8eb1d2b17e15224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3c6e5e61864a049f3eb23bc324bcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a3b67cf9314044821accb541f68d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bfb2f47ac54b1db954c54c484966e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aaac2659d424a899b2d1c864a68f078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565609b6fc894250adefbbfd981a6a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596d20db0c564b74b462db11477737e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90291fd256234d5aa43a846533707104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1050ccf3e43d4ca9bc1952ea69150f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558ff8a498e347e29774419d6c37cfa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880d77cfcdb3402b826688a8563b18c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38cb1680fc24c399d5c22c074adecb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ff4588bb6549c2b1097fc062793da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661538511ccb4dd5838604a5e5e84b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36156f051a0144d2b0e8bc875e79bada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52945ce0e1e4bfe9af1541da291d08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd19e1ae598145bb82f4e862d526c257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b13efbff3cc416dbd833d1dad9bfc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eff58f211b54ed5b83bb2b97895f2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8b22b761c4451baf5b1714f7610fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27a0bd7134a43bfb5cc53fb166dce9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c687f3b59c448fac59bcd9bfef022d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c0033b71024833a11ea1aea158e76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cae209170e4d1bb4f1c78f3063bae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3107f0b66531427a86a8eccf35ae0e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21a34dccd96412da86622e5177c5919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed29a677b7948cf87ee6d6cfd24af41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805de8215c544253bd61462c2c1f60e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f279be48a34d6292b8b2562d2a2ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dc0eecfe5549b8803a2b47889e1e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8216d2921f5b45929890c3cf586542aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: \n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "INFO:lightning.pytorch.utilities.rank_zero:\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    }
   ],
   "source": [
    "# from timm.data import resolve_data_config\n",
    "# from timm.layers import apply_test_time_pool\n",
    "import mlflow.pytorch\n",
    "\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='resnet-50-dropout-l2reg2-augs-adjsmooth2-full-{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=[earlystop_cb, checkpoint_cb],\n",
    "    accelerator='auto',  # GPU if available\n",
    "    precision='16-mixed',\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "model = ResNet50Classifier(lr=1e-4, num_classes=5, smoothing=0.2, window_size=2)\n",
    "# checkpoint = torch.load('checkpoints/resnet-50-dropout-l2reg-augs-adjsmooth-full-epoch=22-val_loss=0.7101-val_acc=0.8264.ckpt')\n",
    "# model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "\n",
    "# # Weird TIMM thing to deal with varied image sizes\n",
    "# data_config = resolve_data_config({}, model=model)\n",
    "# data_config['input_size'] = [3, IMAGE_SIZE_VAL, IMAGE_SIZE_VAL]\n",
    "# model, using_test_pool = apply_test_time_pool(model, data_config)\n",
    "# if using_test_pool:\n",
    "#     print(\"✅ Test-Time Pool head is in place.\")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "print(\"✅ Best checkpoint:\", checkpoint_cb.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b447a",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ab407e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtRegressor(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"convnext_small.fb_in22k_ft_in1k_384\", lr=1e-4, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=True,    # We will manually load weights later\n",
    "            num_classes=1,        # One output neuron for regression\n",
    "            \n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)  # Output shape [batch_size]\n",
    "    \n",
    "    def predict_class(self, outputs):\n",
    "        return outputs.round().clamp(0, self.hparams.num_classes - 1).long()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self(imgs)\n",
    "        loss = F.mse_loss(preds, labels.float())\n",
    "\n",
    "        preds_rounded = preds.round().clamp(0, self.hparams.num_classes - 1)\n",
    "        acc = (preds_rounded == labels).float().mean()\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train_acc',  acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self(imgs)\n",
    "        loss = F.mse_loss(preds, labels.float())\n",
    "\n",
    "        preds_rounded = preds.round().clamp(0, self.hparams.num_classes - 1)\n",
    "        acc = (preds_rounded == labels).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc',  acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=1e-5)\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',          # still minimizing val_loss (MSE)\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6,\n",
    "                verbose=True\n",
    "            ),\n",
    "            'monitor': 'val_loss',   # watch val_loss (MSE) to reduce LR\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_classifier_ckpt(cls, path, model_name=\"convnext_small.fb_in22k_ft_in1k_384\",\n",
    "                                  lr=1e-4, num_classes=5):\n",
    "        \"\"\"\n",
    "        Create a ConvNeXtRegressor and load weights from a classification checkpoint.\n",
    "        \"\"\"\n",
    "        model = cls(model_name=model_name, lr=lr)\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "\n",
    "        state_dict = checkpoint['state_dict']\n",
    "\n",
    "        # Remove classification head weights (they don't match)\n",
    "        filtered_state_dict = {k: v for k, v in state_dict.items() if 'head' not in k}\n",
    "        model.load_state_dict(filtered_state_dict, strict=False)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0e7ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Regressor(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"resnet50.a1_in1k\", lr=1e-4, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=True,    # We will manually load weights later\n",
    "            num_classes=1,        # One output neuron for regression\n",
    "            \n",
    "            drop_rate=0.3,\n",
    "            # drop_path_rate=0.2,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)  # Output shape [batch_size]\n",
    "    \n",
    "    def predict_class(self, outputs):\n",
    "        return outputs.round().clamp(0, self.hparams.num_classes - 1).long()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self(imgs)\n",
    "        loss = F.mse_loss(preds, labels.float())\n",
    "\n",
    "        preds_rounded = preds.round().clamp(0, self.hparams.num_classes - 1)\n",
    "        acc = (preds_rounded == labels).float().mean()\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train_acc',  acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self(imgs)\n",
    "        loss = F.mse_loss(preds, labels.float())\n",
    "\n",
    "        preds_rounded = preds.round().clamp(0, self.hparams.num_classes - 1)\n",
    "        acc = (preds_rounded == labels).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc',  acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=1e-4)\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',          # still minimizing val_loss (MSE)\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6,\n",
    "                verbose=True\n",
    "            ),\n",
    "            'monitor': 'val_loss',   # watch val_loss (MSE) to reduce LR\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_classifier_ckpt(cls, path, model_name=\"convnext_small.fb_in22k_ft_in1k_384\",\n",
    "                                  lr=1e-4, num_classes=5):\n",
    "        \"\"\"\n",
    "        Create a ConvNeXtRegressor and load weights from a classification checkpoint.\n",
    "        \"\"\"\n",
    "        model = cls(model_name=model_name, lr=lr)\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "\n",
    "        state_dict = checkpoint['state_dict']\n",
    "\n",
    "        # Remove classification head weights (they don't match)\n",
    "        filtered_state_dict = {k: v for k, v in state_dict.items() if 'fc' not in k}\n",
    "        model.load_state_dict(filtered_state_dict, strict=False)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75540aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "2025/04/29 00:50:37 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'fdb077ae059f40008c9cac72037baf2f', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pytorch workflow\n",
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /mnt/g/Kaggle-Diabetic-Retinopathy/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type   | Params | Mode \n",
      "----------------------------------------\n",
      "0 | net  | ResNet | 23.5 M | train\n",
      "----------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.040    Total estimated model params size (MB)\n",
      "217       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03589883dae943d6892d0df204ed97dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae37e1a939b478795a26abb1a6e5c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcc02a1ac4c4c7c8545200b76bbc171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f7211b9bab41b09d0d1df83c826da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aca62ff72a346ba967737ac544db3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f8f168dbb34e7ebb1ac7c1bdfbb64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81040a02ec664542abab86e923384b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f733700f595a4d7ab8507e8cc3f664ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833137249c174ebb88881297c657778c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626d8bfb7c974a3f95277868addb6461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ff808d72d24f07b57104f9367f9c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b9b66079ff4512a5a1c510bf6a155a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8b2ee4b1b74ccc8e0b3bbdb73523ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23a300fb3654ac3ae9c1eca2eed4c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f9bc27df604d1192859e1668dcbbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a1072373874d92ad078f96335226d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41743f077545438789b4d169f3f203ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a924af7b52bc492496d4c0517bee787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a5b4de96b848ee9189730e32e223d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388ca8ebba834fd79d4af72cbffa68c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374233108eac4a4f8e93b10b88bdaa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b6ab470e264c22a655d77143dadf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e0a4ace29742d0b7139d409f0d00e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed73570fd7ba4ad4a6aba92cc80fab8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2679fba817a34c2f90c4a334bf84cc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9537ea483d4943b52cf31655fefcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952e7fb3c62d4b1286f5d77698087bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edc491f318d49d684345c6ea437c020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e00300f49e1482ca686c98ef65578da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c06c2c04b64ea5887e181197c4b19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c06d6f50c647fba89fa14ed7ec6417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23d93167caf4892b9088974a1169fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ee0ceeb7ea4d908d6eee96c25a7316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebca8ccb02634a2fb9df1e23671e0b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4467ca139d47bea9eb13464f5ebfc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b6f30d27694d578fe500ab15b8cffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc1e7b29fa9434dbca037e4c1f65d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211e660a35e24266a74e967a8746160c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05720b8d96004f0a9bcdb94a2e952519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4989146f094db5b141eb89df907b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab5643ac8244314ad54e1e709fa6991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a98367257694fe5907985ef8f5d24b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cba636423a42fa8da200bb67c91c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b6d218bc054dd5a7cf4d9a02f73036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/04/29 06:45:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best checkpoint: /mnt/g/Kaggle-Diabetic-Retinopathy/checkpoints/resnet-50-reg-dropout-l2reg-augs-full-epoch=26-val_loss=0.3978-val_acc=0.7151.ckpt\n"
     ]
    }
   ],
   "source": [
    "# from timm.data import resolve_data_config\n",
    "# from timm.layers import apply_test_time_pool\n",
    "import mlflow.pytorch\n",
    "\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='resnet-50-reg-dropout-l2reg-augs-full-{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=[earlystop_cb, checkpoint_cb],\n",
    "    accelerator='auto',  # GPU if available\n",
    "    precision='16-mixed',\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "model = ResNet50Regressor(lr=1e-4, num_classes=5)\n",
    "\n",
    "# # Weird TIMM thing to deal with varied image sizes\n",
    "# data_config = resolve_data_config({}, model=model)\n",
    "# data_config['input_size'] = [3, IMAGE_SIZE_VAL, IMAGE_SIZE_VAL]\n",
    "# model, using_test_pool = apply_test_time_pool(model, data_config)\n",
    "# if using_test_pool:\n",
    "#     print(\"✅ Test-Time Pool head is in place.\")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "print(\"✅ Best checkpoint:\", checkpoint_cb.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4c83b",
   "metadata": {},
   "source": [
    "========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8c62b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'convnext':   ('checkpoints/convnext-small-dropout-l2reg-augs-adjsmooth-full-refined-epoch=00-val_loss=0.6888-val_acc=0.8396.ckpt',\n",
    "                   ConvNeXtClassifier),\n",
    "    'convnext-r': ('checkpoints/convnext-reg-dropout-l2reg-augs-full-epoch=23-val_loss=0.2984-val_acc=0.8075.ckpt',\n",
    "                   ConvNeXtRegressor),\n",
    "    'effnet':     ('checkpoints/effnet-v2rw-m-dropout-l2reg-augs-adjsmooth-full-epoch=14-val_loss=0.7004-val_acc=0.8305.ckpt',\n",
    "                   EfficientNetV2Classifier),\n",
    "    'effnet-p':   ('checkpoints/effnet-v2rw-m-dropout-l2reg-augs-adjsmooth-epoch=12-val_loss=0.6849-val_acc=0.8370.ckpt',\n",
    "                   EfficientNetV2Classifier),\n",
    "    'effnet-o':   ('checkpoints/effnet-v2rw-m-ordinal-dropout2-l2reg2-augs-full-epoch=22-val_loss=0.1580-val_acc=0.8278.ckpt',\n",
    "                   EfficientNetV2OrdinalClassifier),\n",
    "    'resnet':     ('checkpoints/resnet-50-dropout-l2reg2-augs-adjsmooth2-full-epoch=39-val_loss=0.8696-val_acc=0.8299.ckpt',\n",
    "                   ResNet50Classifier),\n",
    "    'resnet-n':   ('checkpoints/resnet-50-dropout-l2reg-augs-adjsmooth-full-refined-epoch=00-val_loss=0.7068-val_acc=0.8268.ckpt',\n",
    "                   ResNet50Classifier),\n",
    "    'resnet-r':   ('checkpoints/resnet-50-reg-dropout-l2reg-augs-full-epoch=26-val_loss=0.3978-val_acc=0.7151.ckpt',\n",
    "                   ResNet50Regressor),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ckpt_path, model_class = models['resnet-r']\n",
    "model = model_class.load_from_checkpoint(ckpt_path, strict=False)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862243b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4774ad74f96d4131ad7a9f62b5647697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(val_dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        with torch.amp.autocast(device.type):\n",
    "            preds_raw = model(imgs)\n",
    "        preds = model.predict_class(preds_raw)\n",
    "\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "# Concatenate into single tensors\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7598329d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext\n",
      "Validation QWK = 0.81847\n",
      "Validation Accuracy = 0.83963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print('convnext')\n",
    "print(f'Validation QWK = {qwk:.5f}')\n",
    "print(f'Validation Accuracy = {acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29f2955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext-r\n",
      "Validation QWK = 0.82712\n",
      "Validation Accuracy = 0.80778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print('convnext-r')\n",
    "print(f'Validation QWK = {qwk:.5f}')\n",
    "print(f'Validation Accuracy = {acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39cd8662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effnet\n",
      "Validation QWK = 0.75777\n",
      "Validation Accuracy = 0.82858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print('effnet')\n",
    "print(f'Validation QWK = {qwk:.5f}')\n",
    "print(f'Validation Accuracy = {acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6b66e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effnet-p\n",
      "Validation QWK = 0.76999\n",
      "Validation Accuracy = 0.82974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print('effnet-p')\n",
    "print(f'Validation QWK = {qwk:.5f}')\n",
    "print(f'Validation Accuracy = {acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "15f597dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effnet-o\n",
      "Validation QWK = 0.80779\n",
      "Validation Accuracy = 0.82396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print('effnet-o')\n",
    "print(f'Validation QWK = {qwk:.5f}')\n",
    "print(f'Validation Accuracy = {acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32e0888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet\n",
      "Validation QWK = 0.76418\n",
      "Validation Accuracy = 0.82987\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print('resnet')\n",
    "print(f'Validation QWK = {qwk:.5f}')\n",
    "print(f'Validation Accuracy = {acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7113bf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet-n\n",
      "Validation QWK = 0.77617\n",
      "Validation Accuracy = 0.82678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print('resnet-n')\n",
    "print(f'Validation QWK = {qwk:.5f}')\n",
    "print(f'Validation Accuracy = {acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dd20054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet-r\n",
      "Validation QWK = 0.72849\n",
      "Validation Accuracy = 0.71507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "qwk = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print('resnet-r')\n",
    "print(f'Validation QWK = {qwk:.5f}')\n",
    "print(f'Validation Accuracy = {acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6282a57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing convnext…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['criterion.weight_matrix']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16814ea271a94b2d8af2d51cae8cb9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing convnext-r…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084752c64fd042d5ae6b0032715e9834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing effnet…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['criterion.weight_matrix']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf4ef0cab60463ca2c1a034527ab128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing effnet-p…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['criterion.weight_matrix']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f8ad2297794e439b716791bb9b1e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing effnet-o…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03638a783792463d9fb6bc579b37e735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing resnet…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecc205fd47b4145b254b1962aeb55a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing resnet-n…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['criterion.weight_matrix']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9819fcdbd74b1aaeb8130a6678abb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing resnet-r…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcb4b05da834588bc00d1708a022b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assume that val_dataloader does NOT shuffle, so order matches df_val\n",
    "\n",
    "# Prepare an empty DataFrame\n",
    "df_stack = pd.DataFrame()\n",
    "\n",
    "# Loop each model\n",
    "for model_name, (ckpt_path, model_class) in models.items():\n",
    "    print(f'Processing {model_name}…')\n",
    "\n",
    "    # 1) load & move to device\n",
    "    model = model_class.load_from_checkpoint(ckpt_path, strict=False)\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # 2) collect raw outputs in a list\n",
    "    all_out = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in tqdm(val_dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            with torch.amp.autocast(device.type):\n",
    "                out = model(imgs)\n",
    "            # out: Tensor of shape [B] or [B, D]\n",
    "            all_out.append(out.cpu())\n",
    "\n",
    "    # 3) concatenate and convert to numpy\n",
    "    all_out = torch.cat(all_out, dim=0).numpy()  # shape (N,) or (N, D)\n",
    "\n",
    "    # 4) turn into stacking columns\n",
    "    if all_out.ndim == 1:\n",
    "        # regressor → one column\n",
    "        df_stack[f'{model_name}_pred'] = all_out\n",
    "    else:\n",
    "        # multi‐dim output → one column per dim\n",
    "        D = all_out.shape[1]\n",
    "        for i in range(D):\n",
    "            df_stack[f'{model_name}_{i}'] = all_out[:, i]\n",
    "\n",
    "    # cleanup\n",
    "    del model\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# finally, append the true label column (order must match val_dataloader)\n",
    "df_stack['label'] = df_val['label'].values\n",
    "df_stack.to_csv('df_stack.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690b5e6",
   "metadata": {},
   "source": [
    "========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c25b6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from tabpfn import TabPFNRegressor\n",
    "\n",
    "import joblib\n",
    "\n",
    "df_stack = pd.read_csv('df_stack.csv')\n",
    "num_classes = int(df_stack['label'].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b0519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def cv_score(feature_columns, model_, kf, num_classes=5, fit_kwargs={}):\n",
    "\n",
    "    qwk_scores = []\n",
    "    models_trained = []\n",
    "    \n",
    "    for fold, (tr_idx, te_idx) in enumerate(kf.split(df_stack), 1):\n",
    "        X_tr = df_stack.iloc[tr_idx][feature_columns]\n",
    "        y_tr = df_stack.iloc[tr_idx]['label']\n",
    "        X_te = df_stack.iloc[te_idx][feature_columns]\n",
    "        y_te = df_stack.iloc[te_idx]['label']\n",
    "\n",
    "        model = copy.deepcopy(model_)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_te, y_te)], **fit_kwargs)\n",
    "\n",
    "        preds = model.predict(X_te)\n",
    "        preds_int = np.clip(np.rint(preds), 0, num_classes-1).astype(int)\n",
    "        qwk = cohen_kappa_score(y_te, preds_int, weights='quadratic')\n",
    "        \n",
    "        qwk_scores.append(qwk)\n",
    "        models_trained.append(copy.deepcopy(model))\n",
    "        del model\n",
    "\n",
    "    return float(np.mean(qwk_scores)), models_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e3ee54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Running baseline 5-fold CV on all features …\n",
      "Baseline mean QWK: 0.8352\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1415015f3f5c47e6af54899ffc52001d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8375\n",
      "  Removing convnext-r   → mean QWK = 0.8391\n",
      "  Removing effnet       → mean QWK = 0.8371\n",
      "  Removing effnet-o     → mean QWK = 0.8399\n",
      "  Removing effnet-p     → mean QWK = 0.8317\n",
      "  Removing resnet       → mean QWK = 0.8361\n",
      "  Removing resnet-n     → mean QWK = 0.8392\n",
      "  Removing resnet-r     → mean QWK = 0.8384\n",
      "\n",
      "✔ Eliminating model 'effnet-o' improved QWK: 0.8352 → 0.8399\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab04c8bed3ce484f9fabcd982d0aaacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8347\n",
      "  Removing convnext-r   → mean QWK = 0.8389\n",
      "  Removing effnet       → mean QWK = 0.8410\n",
      "  Removing effnet-p     → mean QWK = 0.8333\n",
      "  Removing resnet       → mean QWK = 0.8362\n",
      "  Removing resnet-n     → mean QWK = 0.8391\n",
      "  Removing resnet-r     → mean QWK = 0.8375\n",
      "\n",
      "✔ Eliminating model 'effnet' improved QWK: 0.8399 → 0.8410\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae2d726fa8046d98dbe78cb1b8f3211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8350\n",
      "  Removing convnext-r   → mean QWK = 0.8370\n",
      "  Removing effnet-p     → mean QWK = 0.8306\n",
      "  Removing resnet       → mean QWK = 0.8371\n",
      "  Removing resnet-n     → mean QWK = 0.8380\n",
      "  Removing resnet-r     → mean QWK = 0.8388\n",
      "\n",
      "— No single-model removal improved QWK. Elimination complete.\n",
      "\n",
      "▶ Training final 5-fold models on features from: ['convnext', 'convnext-r', 'effnet-p', 'resnet', 'resnet-n', 'resnet-r']\n",
      "Final mean QWK: 0.8410\n",
      "\n",
      "✔ Saved fold 1 model to 'stacking_models/stack-lgb_cpu-fold1.pkl'\n",
      "✔ Saved fold 2 model to 'stacking_models/stack-lgb_cpu-fold2.pkl'\n",
      "✔ Saved fold 3 model to 'stacking_models/stack-lgb_cpu-fold3.pkl'\n",
      "✔ Saved fold 4 model to 'stacking_models/stack-lgb_cpu-fold4.pkl'\n",
      "✔ Saved fold 5 model to 'stacking_models/stack-lgb_cpu-fold5.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Identify models by splitting on '_'\n",
    "feature_cols = [c for c in df_stack.columns if c != 'label']\n",
    "models = sorted({c.split('_')[0] for c in feature_cols})\n",
    "\n",
    "# Map each model name → list of its feature columns\n",
    "features_by_model = {\n",
    "    m: [c for c in feature_cols if c.startswith(m + '_')]\n",
    "    for m in models\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "stacking_model = LGBMRegressor(verbose=0)\n",
    "\n",
    "# --- Baseline\n",
    "print(\"▶ Running baseline 5-fold CV on all features …\")\n",
    "base_mean, base_models = cv_score(feature_cols, stacking_model, kf, num_classes)\n",
    "print(f\"Baseline mean QWK: {base_mean:.4f}\\n\")\n",
    "\n",
    "current_models = models.copy()\n",
    "current_features = feature_cols.copy()\n",
    "best_mean = base_mean\n",
    "\n",
    "# --- Backward elimination\n",
    "while True:\n",
    "    print(\"▶ Testing removal of each model …\")\n",
    "    removal_results = {}\n",
    "    \n",
    "    for m in tqdm(current_models, desc=\"Models\"):\n",
    "        # drop m’s features\n",
    "        feat = [c for c in current_features if not c.startswith(m + '_')]\n",
    "        mean_qwk, _ = cv_score(feat, stacking_model, kf, num_classes)\n",
    "        removal_results[m] = mean_qwk\n",
    "        print(f\"  Removing {m:12} → mean QWK = {mean_qwk:.4f}\")\n",
    "\n",
    "    # find best improvement\n",
    "    worst_model, candidate_qwk = max(removal_results.items(), key=lambda kv: kv[1])\n",
    "    if candidate_qwk > best_mean:\n",
    "        print(f\"\\n✔ Eliminating model '{worst_model}' improved QWK: {best_mean:.4f} → {candidate_qwk:.4f}\\n\")\n",
    "        # update state\n",
    "        best_mean = candidate_qwk\n",
    "        current_models.remove(worst_model)\n",
    "        current_features = [c for c in current_features if not c.startswith(worst_model + '_')]\n",
    "    else:\n",
    "        print(\"\\n— No single-model removal improved QWK. Elimination complete.\\n\")\n",
    "        break\n",
    "\n",
    "# --- Final training on selected features\n",
    "print(\"▶ Training final 5-fold models on features from:\", current_models)\n",
    "final_mean, final_models = cv_score(current_features, stacking_model, kf, num_classes)\n",
    "print(f\"Final mean QWK: {final_mean:.4f}\\n\")\n",
    "\n",
    "# --- Save each model\n",
    "for i, mdl in enumerate(final_models, 1):\n",
    "    path = f\"stacking_models/stack-lgb_cpu-fold{i}.pkl\"\n",
    "    joblib.dump(mdl, path)\n",
    "    print(f\"✔ Saved fold {i} model to '{path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2966303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Running baseline 5-fold CV on all features …\n",
      "Baseline mean QWK: 0.8348\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800d46c404a44d268c7b015d6c4f812d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8363\n",
      "  Removing convnext-r   → mean QWK = 0.8401\n",
      "  Removing effnet       → mean QWK = 0.8374\n",
      "  Removing effnet-o     → mean QWK = 0.8385\n",
      "  Removing effnet-p     → mean QWK = 0.8317\n",
      "  Removing resnet       → mean QWK = 0.8364\n",
      "  Removing resnet-n     → mean QWK = 0.8388\n",
      "  Removing resnet-r     → mean QWK = 0.8384\n",
      "\n",
      "✔ Eliminating model 'convnext-r' improved QWK: 0.8348 → 0.8401\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179434b51c97478591a7fb3a80d971d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8370\n",
      "  Removing effnet       → mean QWK = 0.8379\n",
      "  Removing effnet-o     → mean QWK = 0.8393\n",
      "  Removing effnet-p     → mean QWK = 0.8318\n",
      "  Removing resnet       → mean QWK = 0.8385\n",
      "  Removing resnet-n     → mean QWK = 0.8388\n",
      "  Removing resnet-r     → mean QWK = 0.8383\n",
      "\n",
      "— No single-model removal improved QWK. Elimination complete.\n",
      "\n",
      "▶ Training final 5-fold models on features from: ['convnext', 'effnet', 'effnet-o', 'effnet-p', 'resnet', 'resnet-n', 'resnet-r']\n",
      "Final mean QWK: 0.8401\n",
      "\n",
      "✔ Saved fold 1 model to 'stacking_models/stack-lgb_cuda-fold1.pkl'\n",
      "✔ Saved fold 2 model to 'stacking_models/stack-lgb_cuda-fold2.pkl'\n",
      "✔ Saved fold 3 model to 'stacking_models/stack-lgb_cuda-fold3.pkl'\n",
      "✔ Saved fold 4 model to 'stacking_models/stack-lgb_cuda-fold4.pkl'\n",
      "✔ Saved fold 5 model to 'stacking_models/stack-lgb_cuda-fold5.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Identify models by splitting on '_'\n",
    "feature_cols = [c for c in df_stack.columns if c != 'label']\n",
    "models = sorted({c.split('_')[0] for c in feature_cols})\n",
    "\n",
    "# Map each model name → list of its feature columns\n",
    "features_by_model = {\n",
    "    m: [c for c in feature_cols if c.startswith(m + '_')]\n",
    "    for m in models\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "stacking_model = LGBMRegressor(verbose=-1, device='cuda')\n",
    "\n",
    "# --- Baseline\n",
    "print(\"▶ Running baseline 5-fold CV on all features …\")\n",
    "base_mean, base_models = cv_score(feature_cols, stacking_model, kf, num_classes)\n",
    "print(f\"Baseline mean QWK: {base_mean:.4f}\\n\")\n",
    "\n",
    "current_models = models.copy()\n",
    "current_features = feature_cols.copy()\n",
    "best_mean = base_mean\n",
    "\n",
    "# --- Backward elimination\n",
    "while True:\n",
    "    print(\"▶ Testing removal of each model …\")\n",
    "    removal_results = {}\n",
    "    \n",
    "    for m in tqdm(current_models, desc=\"Models\"):\n",
    "        # drop m’s features\n",
    "        feat = [c for c in current_features if not c.startswith(m + '_')]\n",
    "        mean_qwk, _ = cv_score(feat, stacking_model, kf, num_classes)\n",
    "        removal_results[m] = mean_qwk\n",
    "        print(f\"  Removing {m:12} → mean QWK = {mean_qwk:.4f}\")\n",
    "\n",
    "    # find best improvement\n",
    "    worst_model, candidate_qwk = max(removal_results.items(), key=lambda kv: kv[1])\n",
    "    if candidate_qwk > best_mean:\n",
    "        print(f\"\\n✔ Eliminating model '{worst_model}' improved QWK: {best_mean:.4f} → {candidate_qwk:.4f}\\n\")\n",
    "        # update state\n",
    "        best_mean = candidate_qwk\n",
    "        current_models.remove(worst_model)\n",
    "        current_features = [c for c in current_features if not c.startswith(worst_model + '_')]\n",
    "    else:\n",
    "        print(\"\\n— No single-model removal improved QWK. Elimination complete.\\n\")\n",
    "        break\n",
    "\n",
    "# --- Final training on selected features\n",
    "print(\"▶ Training final 5-fold models on features from:\", current_models)\n",
    "final_mean, final_models = cv_score(current_features, stacking_model, kf, num_classes)\n",
    "print(f\"Final mean QWK: {final_mean:.4f}\\n\")\n",
    "\n",
    "# --- Save each model\n",
    "for i, mdl in enumerate(final_models, 1):\n",
    "    path = f\"stacking_models/stack-lgb_cuda-fold{i}.pkl\"\n",
    "    joblib.dump(mdl, path)\n",
    "    print(f\"✔ Saved fold {i} model to '{path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66269d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "class FilteredStream:\n",
    "    def __init__(self, stream, pattern):\n",
    "        self.stream = stream\n",
    "        self.pattern = re.compile(pattern)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def write(self, text):\n",
    "        self.buffer += text\n",
    "        lines = self.buffer.splitlines(True)\n",
    "        for line in lines:\n",
    "            if line.endswith('\\n'):\n",
    "                if not self.pattern.search(line):\n",
    "                    self.stream.write(line)\n",
    "            else:\n",
    "                self.buffer = line\n",
    "                break\n",
    "        else:\n",
    "            self.buffer = ''\n",
    "\n",
    "    def flush(self):\n",
    "        if self.buffer:\n",
    "            if not self.pattern.search(self.buffer):\n",
    "                self.stream.write(self.buffer)\n",
    "            self.buffer = ''\n",
    "        self.stream.flush()\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.stream, attr)\n",
    "\n",
    "# Define the fucking pattern to kill\n",
    "pattern = r'\\[\\d+\\]\\s+validation_0-rmse:\\d+\\.\\d+'\n",
    "\n",
    "# Save the original streams\n",
    "original_stdout = sys.stdout\n",
    "original_stderr = sys.stderr\n",
    "\n",
    "# Set the filtered streams\n",
    "sys.stdout = FilteredStream(original_stdout, pattern)\n",
    "sys.stderr = FilteredStream(original_stderr, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Running baseline 5-fold CV on all features …\n",
      "Baseline mean QWK: 0.8352\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54aa6ea8cde84f5183ff985014752db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8309\n",
      "  Removing convnext-r   → mean QWK = 0.8296\n",
      "  Removing effnet       → mean QWK = 0.8349\n",
      "  Removing effnet-o     → mean QWK = 0.8319\n",
      "  Removing effnet-p     → mean QWK = 0.8256\n",
      "  Removing resnet       → mean QWK = 0.8345\n",
      "  Removing resnet-n     → mean QWK = 0.8326\n",
      "  Removing resnet-r     → mean QWK = 0.8354\n",
      "\n",
      "✔ Eliminating model 'resnet-r' improved QWK: 0.8352 → 0.8354\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d60dde3258d410285775fa0888de022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8362\n",
      "  Removing convnext-r   → mean QWK = 0.8322\n",
      "  Removing effnet       → mean QWK = 0.8338\n",
      "  Removing effnet-o     → mean QWK = 0.8309\n",
      "  Removing effnet-p     → mean QWK = 0.8282\n",
      "  Removing resnet       → mean QWK = 0.8309\n",
      "  Removing resnet-n     → mean QWK = 0.8334\n",
      "\n",
      "✔ Eliminating model 'convnext' improved QWK: 0.8354 → 0.8362\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a630d116ad4b9bbe443d0698503e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext-r   → mean QWK = 0.8273\n",
      "  Removing effnet       → mean QWK = 0.8341\n",
      "  Removing effnet-o     → mean QWK = 0.8328\n",
      "  Removing effnet-p     → mean QWK = 0.8260\n",
      "  Removing resnet       → mean QWK = 0.8360\n",
      "  Removing resnet-n     → mean QWK = 0.8316\n",
      "\n",
      "— No single-model removal improved QWK. Elimination complete.\n",
      "\n",
      "▶ Training final 5-fold models on features from: ['convnext-r', 'effnet', 'effnet-o', 'effnet-p', 'resnet', 'resnet-n']\n",
      "Final mean QWK: 0.8362\n",
      "\n",
      "✔ Saved fold 1 model to 'stacking_models/stack-xgb-fold1.pkl'\n",
      "✔ Saved fold 2 model to 'stacking_models/stack-xgb-fold2.pkl'\n",
      "✔ Saved fold 3 model to 'stacking_models/stack-xgb-fold3.pkl'\n",
      "✔ Saved fold 4 model to 'stacking_models/stack-xgb-fold4.pkl'\n",
      "✔ Saved fold 5 model to 'stacking_models/stack-xgb-fold5.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Identify models by splitting on '_'\n",
    "feature_cols = [c for c in df_stack.columns if c != 'label']\n",
    "models = sorted({c.split('_')[0] for c in feature_cols})\n",
    "\n",
    "# Map each model name → list of its feature columns\n",
    "features_by_model = {\n",
    "    m: [c for c in feature_cols if c.startswith(m + '_')]\n",
    "    for m in models\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "stacking_model = XGBRegressor(verbosity=0, device='gpu',\n",
    "                              n_estimators=1000, early_stopping_rounds=100)\n",
    "                            #   learning_rate=0.05)\n",
    "\n",
    "# --- Baseline\n",
    "print(\"▶ Running baseline 5-fold CV on all features …\")\n",
    "base_mean, base_models = cv_score(feature_cols, stacking_model, kf, num_classes)\n",
    "print(f\"Baseline mean QWK: {base_mean:.4f}\\n\")\n",
    "\n",
    "current_models = models.copy()\n",
    "current_features = feature_cols.copy()\n",
    "best_mean = base_mean\n",
    "\n",
    "# --- Backward elimination\n",
    "while True:\n",
    "    print(\"▶ Testing removal of each model …\")\n",
    "    removal_results = {}\n",
    "    \n",
    "    for m in tqdm(current_models, desc=\"Models\"):\n",
    "        # drop m’s features\n",
    "        feat = [c for c in current_features if not c.startswith(m + '_')]\n",
    "        mean_qwk, _ = cv_score(feat, stacking_model, kf, num_classes)\n",
    "        removal_results[m] = mean_qwk\n",
    "        print(f\"  Removing {m:12} → mean QWK = {mean_qwk:.4f}\")\n",
    "\n",
    "    # find best improvement\n",
    "    worst_model, candidate_qwk = max(removal_results.items(), key=lambda kv: kv[1])\n",
    "    if candidate_qwk > best_mean:\n",
    "        print(f\"\\n✔ Eliminating model '{worst_model}' improved QWK: {best_mean:.4f} → {candidate_qwk:.4f}\\n\")\n",
    "        # update state\n",
    "        best_mean = candidate_qwk\n",
    "        current_models.remove(worst_model)\n",
    "        current_features = [c for c in current_features if not c.startswith(worst_model + '_')]\n",
    "    else:\n",
    "        print(\"\\n— No single-model removal improved QWK. Elimination complete.\\n\")\n",
    "        break\n",
    "\n",
    "# --- Final training on selected features\n",
    "print(\"▶ Training final 5-fold models on features from:\", current_models)\n",
    "final_mean, final_models = cv_score(current_features, stacking_model, kf, num_classes)\n",
    "print(f\"Final mean QWK: {final_mean:.4f}\\n\")\n",
    "\n",
    "# --- Save each model\n",
    "for i, mdl in enumerate(final_models, 1):\n",
    "    path = f\"stacking_models/stack-xgb-fold{i}.pkl\"\n",
    "    joblib.dump(mdl, path)\n",
    "    print(f\"✔ Saved fold {i} model to '{path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2997f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Running baseline 5-fold CV on all features …\n",
      "Baseline mean QWK: 0.8370\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fe265be2ed4779af5f9bd996684d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8383\n",
      "  Removing convnext-r   → mean QWK = 0.8382\n",
      "  Removing effnet       → mean QWK = 0.8399\n",
      "  Removing effnet-o     → mean QWK = 0.8406\n",
      "  Removing effnet-p     → mean QWK = 0.8348\n",
      "  Removing resnet       → mean QWK = 0.8406\n",
      "  Removing resnet-n     → mean QWK = 0.8407\n",
      "  Removing resnet-r     → mean QWK = 0.8392\n",
      "\n",
      "✔ Eliminating model 'resnet-n' improved QWK: 0.8370 → 0.8407\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd0ec1c425d4d6d8936353c023cbc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8373\n",
      "  Removing convnext-r   → mean QWK = 0.8410\n",
      "  Removing effnet       → mean QWK = 0.8398\n",
      "  Removing effnet-o     → mean QWK = 0.8383\n",
      "  Removing effnet-p     → mean QWK = 0.8324\n",
      "  Removing resnet       → mean QWK = 0.8420\n",
      "  Removing resnet-r     → mean QWK = 0.8367\n",
      "\n",
      "✔ Eliminating model 'resnet' improved QWK: 0.8407 → 0.8420\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e33120a442442fa6a3385d61897326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8379\n",
      "  Removing convnext-r   → mean QWK = 0.8407\n",
      "  Removing effnet       → mean QWK = 0.8413\n",
      "  Removing effnet-o     → mean QWK = 0.8395\n",
      "  Removing effnet-p     → mean QWK = 0.8342\n",
      "  Removing resnet-r     → mean QWK = 0.8428\n",
      "\n",
      "✔ Eliminating model 'resnet-r' improved QWK: 0.8420 → 0.8428\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b4644960254592b71a62cb99c81015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8369\n",
      "  Removing convnext-r   → mean QWK = 0.8412\n",
      "  Removing effnet       → mean QWK = 0.8394\n",
      "  Removing effnet-o     → mean QWK = 0.8403\n",
      "  Removing effnet-p     → mean QWK = 0.8342\n",
      "\n",
      "— No single-model removal improved QWK. Elimination complete.\n",
      "\n",
      "▶ Training final 5-fold models on features from: ['convnext', 'convnext-r', 'effnet', 'effnet-o', 'effnet-p']\n",
      "Final mean QWK: 0.8428\n",
      "\n",
      "✔ Saved fold 1 model to 'stack_model_fold1.pkl'\n",
      "✔ Saved fold 2 model to 'stack_model_fold2.pkl'\n",
      "✔ Saved fold 3 model to 'stack_model_fold3.pkl'\n",
      "✔ Saved fold 4 model to 'stack_model_fold4.pkl'\n",
      "✔ Saved fold 5 model to 'stack_model_fold5.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Identify models by splitting on '_'\n",
    "feature_cols = [c for c in df_stack.columns if c != 'label']\n",
    "models = sorted({c.split('_')[0] for c in feature_cols})\n",
    "\n",
    "# Map each model name → list of its feature columns\n",
    "features_by_model = {\n",
    "    m: [c for c in feature_cols if c.startswith(m + '_')]\n",
    "    for m in models\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "stacking_model = CatBoostRegressor(verbose=0)\n",
    "\n",
    "# --- Baseline\n",
    "print(\"▶ Running baseline 5-fold CV on all features …\")\n",
    "base_mean, base_models = cv_score(feature_cols, stacking_model, kf, num_classes)\n",
    "print(f\"Baseline mean QWK: {base_mean:.4f}\\n\")\n",
    "\n",
    "current_models = models.copy()\n",
    "current_features = feature_cols.copy()\n",
    "best_mean = base_mean\n",
    "\n",
    "# --- Backward elimination\n",
    "while True:\n",
    "    print(\"▶ Testing removal of each model …\")\n",
    "    removal_results = {}\n",
    "    \n",
    "    for m in tqdm(current_models, desc=\"Models\"):\n",
    "        # drop m’s features\n",
    "        feat = [c for c in current_features if not c.startswith(m + '_')]\n",
    "        mean_qwk, _ = cv_score(feat, stacking_model, kf, num_classes)\n",
    "        removal_results[m] = mean_qwk\n",
    "        print(f\"  Removing {m:12} → mean QWK = {mean_qwk:.4f}\")\n",
    "\n",
    "    # find best improvement\n",
    "    worst_model, candidate_qwk = max(removal_results.items(), key=lambda kv: kv[1])\n",
    "    if candidate_qwk > best_mean:\n",
    "        print(f\"\\n✔ Eliminating model '{worst_model}' improved QWK: {best_mean:.4f} → {candidate_qwk:.4f}\\n\")\n",
    "        # update state\n",
    "        best_mean = candidate_qwk\n",
    "        current_models.remove(worst_model)\n",
    "        current_features = [c for c in current_features if not c.startswith(worst_model + '_')]\n",
    "    else:\n",
    "        print(\"\\n— No single-model removal improved QWK. Elimination complete.\\n\")\n",
    "        break\n",
    "\n",
    "# --- Final training on selected features\n",
    "print(\"▶ Training final 5-fold models on features from:\", current_models)\n",
    "final_mean, final_models = cv_score(current_features, stacking_model, kf, num_classes)\n",
    "print(f\"Final mean QWK: {final_mean:.4f}\\n\")\n",
    "\n",
    "# --- Save each model\n",
    "for i, mdl in enumerate(final_models, 1):\n",
    "    path = f\"stack-cb_cpu-fold{i}.pkl\"\n",
    "    joblib.dump(mdl, path)\n",
    "    print(f\"✔ Saved fold {i} model to '{path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4008dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Running baseline 5-fold CV on all features …\n",
      "Baseline mean QWK: 0.8442\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60505754191b4865a3af01607b9ccd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8444\n",
      "  Removing convnext-r   → mean QWK = 0.8434\n",
      "  Removing effnet       → mean QWK = 0.8453\n",
      "  Removing effnet-o     → mean QWK = 0.8463\n",
      "  Removing effnet-p     → mean QWK = 0.8377\n",
      "  Removing resnet       → mean QWK = 0.8452\n",
      "  Removing resnet-n     → mean QWK = 0.8442\n",
      "  Removing resnet-r     → mean QWK = 0.8422\n",
      "\n",
      "✔ Eliminating model 'effnet-o' improved QWK: 0.8442 → 0.8463\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775c4b2d756b4c63af0cb3f5fa8c8f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8415\n",
      "  Removing convnext-r   → mean QWK = 0.8431\n",
      "  Removing effnet       → mean QWK = 0.8444\n",
      "  Removing effnet-p     → mean QWK = 0.8365\n",
      "  Removing resnet       → mean QWK = 0.8463\n",
      "  Removing resnet-n     → mean QWK = 0.8451\n",
      "  Removing resnet-r     → mean QWK = 0.8455\n",
      "\n",
      "✔ Eliminating model 'resnet' improved QWK: 0.8463 → 0.8463\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5498a33b8a4b23a47393144d4b92ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8443\n",
      "  Removing convnext-r   → mean QWK = 0.8403\n",
      "  Removing effnet       → mean QWK = 0.8454\n",
      "  Removing effnet-p     → mean QWK = 0.8387\n",
      "  Removing resnet-n     → mean QWK = 0.8446\n",
      "  Removing resnet-r     → mean QWK = 0.8449\n",
      "\n",
      "— No single-model removal improved QWK. Elimination complete.\n",
      "\n",
      "▶ Training final 5-fold models on features from: ['convnext', 'convnext-r', 'effnet', 'effnet-p', 'resnet-n', 'resnet-r']\n",
      "Final mean QWK: 0.8463\n",
      "\n",
      "✔ Saved fold 1 model to 'stack-cb_gpu-fold1.pkl'\n",
      "✔ Saved fold 2 model to 'stack-cb_gpu-fold2.pkl'\n",
      "✔ Saved fold 3 model to 'stack-cb_gpu-fold3.pkl'\n",
      "✔ Saved fold 4 model to 'stack-cb_gpu-fold4.pkl'\n",
      "✔ Saved fold 5 model to 'stack-cb_gpu-fold5.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Identify models by splitting on '_'\n",
    "feature_cols = [c for c in df_stack.columns if c != 'label']\n",
    "models = sorted({c.split('_')[0] for c in feature_cols})\n",
    "\n",
    "# Map each model name → list of its feature columns\n",
    "features_by_model = {\n",
    "    m: [c for c in feature_cols if c.startswith(m + '_')]\n",
    "    for m in models\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "stacking_model = CatBoostRegressor(task_type='GPU',\n",
    "                                   iterations=5000, early_stopping_rounds=100,\n",
    "                                   use_best_model=True,\n",
    "                                   verbose=0)\n",
    "\n",
    "# --- Baseline\n",
    "print(\"▶ Running baseline 5-fold CV on all features …\")\n",
    "base_mean, base_models = cv_score(feature_cols, stacking_model, kf, num_classes)\n",
    "print(f\"Baseline mean QWK: {base_mean:.4f}\\n\")\n",
    "\n",
    "current_models = models.copy()\n",
    "current_features = feature_cols.copy()\n",
    "best_mean = base_mean\n",
    "\n",
    "# --- Backward elimination\n",
    "while True:\n",
    "    print(\"▶ Testing removal of each model …\")\n",
    "    removal_results = {}\n",
    "    \n",
    "    for m in tqdm(current_models, desc=\"Models\"):\n",
    "        # drop m’s features\n",
    "        feat = [c for c in current_features if not c.startswith(m + '_')]\n",
    "        mean_qwk, _ = cv_score(feat, stacking_model, kf, num_classes)\n",
    "        removal_results[m] = mean_qwk\n",
    "        print(f\"  Removing {m:12} → mean QWK = {mean_qwk:.4f}\")\n",
    "\n",
    "    # find best improvement\n",
    "    worst_model, candidate_qwk = max(removal_results.items(), key=lambda kv: kv[1])\n",
    "    if candidate_qwk > best_mean:\n",
    "        print(f\"\\n✔ Eliminating model '{worst_model}' improved QWK: {best_mean:.4f} → {candidate_qwk:.4f}\\n\")\n",
    "        # update state\n",
    "        best_mean = candidate_qwk\n",
    "        current_models.remove(worst_model)\n",
    "        current_features = [c for c in current_features if not c.startswith(worst_model + '_')]\n",
    "    else:\n",
    "        print(\"\\n— No single-model removal improved QWK. Elimination complete.\\n\")\n",
    "        break\n",
    "\n",
    "# --- Final training on selected features\n",
    "print(\"▶ Training final 5-fold models on features from:\", current_models)\n",
    "final_mean, final_models = cv_score(current_features, stacking_model, kf, num_classes)\n",
    "print(f\"Final mean QWK: {final_mean:.4f}\\n\")\n",
    "\n",
    "# --- Save each model\n",
    "for i, mdl in enumerate(final_models, 1):\n",
    "    path = f\"stack-cb_gpu-fold{i}.pkl\"\n",
    "    joblib.dump(mdl, path)\n",
    "    print(f\"✔ Saved fold {i} model to '{path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7a31c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved fold 1 model to 'stack-cb_gpu-fold1.pkl'\n",
      "✔ Saved fold 2 model to 'stack-cb_gpu-fold2.pkl'\n",
      "✔ Saved fold 3 model to 'stack-cb_gpu-fold3.pkl'\n",
      "✔ Saved fold 4 model to 'stack-cb_gpu-fold4.pkl'\n",
      "✔ Saved fold 5 model to 'stack-cb_gpu-fold5.pkl'\n"
     ]
    }
   ],
   "source": [
    "for i, mdl in enumerate(final_models, 1):\n",
    "    path = f\"stack-cb_gpu-fold{i}.pkl\"\n",
    "    joblib.dump(mdl, path)\n",
    "    print(f\"✔ Saved fold {i} model to '{path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85825894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convnext_0',\n",
       " 'convnext_1',\n",
       " 'convnext_2',\n",
       " 'convnext_3',\n",
       " 'convnext_4',\n",
       " 'convnext-r_pred',\n",
       " 'effnet_0',\n",
       " 'effnet_1',\n",
       " 'effnet_2',\n",
       " 'effnet_3',\n",
       " 'effnet_4',\n",
       " 'effnet-p_0',\n",
       " 'effnet-p_1',\n",
       " 'effnet-p_2',\n",
       " 'effnet-p_3',\n",
       " 'effnet-p_4',\n",
       " 'resnet-n_0',\n",
       " 'resnet-n_1',\n",
       " 'resnet-n_2',\n",
       " 'resnet-n_3',\n",
       " 'resnet-n_4',\n",
       " 'resnet-r_pred']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_models[0].feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76991f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Running baseline 5-fold CV on all features …\n",
      "Baseline mean QWK: 0.8428\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee934c0f69bd4e359861a8db01d5b0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8387\n",
      "  Removing convnext-r   → mean QWK = 0.8393\n",
      "  Removing effnet       → mean QWK = 0.8402\n",
      "  Removing effnet-o     → mean QWK = 0.8417\n",
      "  Removing effnet-p     → mean QWK = 0.8345\n",
      "  Removing resnet       → mean QWK = 0.8424\n",
      "  Removing resnet-n     → mean QWK = 0.8431\n",
      "  Removing resnet-r     → mean QWK = 0.8430\n",
      "\n",
      "✔ Eliminating model 'resnet-n' improved QWK: 0.8428 → 0.8431\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4184aa716148a8927d28ab54e7c750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8394\n",
      "  Removing convnext-r   → mean QWK = 0.8414\n",
      "  Removing effnet       → mean QWK = 0.8423\n",
      "  Removing effnet-o     → mean QWK = 0.8399\n",
      "  Removing effnet-p     → mean QWK = 0.8336\n",
      "  Removing resnet       → mean QWK = 0.8441\n",
      "  Removing resnet-r     → mean QWK = 0.8437\n",
      "\n",
      "✔ Eliminating model 'resnet' improved QWK: 0.8431 → 0.8441\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e248e65616794786ac009b8f0dcec284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8384\n",
      "  Removing convnext-r   → mean QWK = 0.8405\n",
      "  Removing effnet       → mean QWK = 0.8411\n",
      "  Removing effnet-o     → mean QWK = 0.8395\n",
      "  Removing effnet-p     → mean QWK = 0.8343\n",
      "  Removing resnet-r     → mean QWK = 0.8431\n",
      "\n",
      "— No single-model removal improved QWK. Elimination complete.\n",
      "\n",
      "▶ Training final 5-fold models on features from: ['convnext', 'convnext-r', 'effnet', 'effnet-o', 'effnet-p', 'resnet-r']\n",
      "Final mean QWK: 0.8441\n",
      "\n",
      "✔ Saved fold 1 model to 'stack-tabpfn-fold1.pkl'\n",
      "✔ Saved fold 2 model to 'stack-tabpfn-fold2.pkl'\n",
      "✔ Saved fold 3 model to 'stack-tabpfn-fold3.pkl'\n",
      "✔ Saved fold 4 model to 'stack-tabpfn-fold4.pkl'\n",
      "✔ Saved fold 5 model to 'stack-tabpfn-fold5.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Identify models by splitting on '_'\n",
    "feature_cols = [c for c in df_stack.columns if c != 'label']\n",
    "models = sorted({c.split('_')[0] for c in feature_cols})\n",
    "\n",
    "# Map each model name → list of its feature columns\n",
    "features_by_model = {\n",
    "    m: [c for c in feature_cols if c.startswith(m + '_')]\n",
    "    for m in models\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "stacking_model = TabPFNRegressor()\n",
    "\n",
    "# --- Baseline\n",
    "print(\"▶ Running baseline 5-fold CV on all features …\")\n",
    "base_mean, base_models = cv_score(feature_cols, stacking_model, kf, num_classes)\n",
    "print(f\"Baseline mean QWK: {base_mean:.4f}\\n\")\n",
    "\n",
    "current_models = models.copy()\n",
    "current_features = feature_cols.copy()\n",
    "best_mean = base_mean\n",
    "\n",
    "# --- Backward elimination\n",
    "while True:\n",
    "    print(\"▶ Testing removal of each model …\")\n",
    "    removal_results = {}\n",
    "    \n",
    "    for m in tqdm(current_models, desc=\"Models\"):\n",
    "        # drop m’s features\n",
    "        feat = [c for c in current_features if not c.startswith(m + '_')]\n",
    "        mean_qwk, _ = cv_score(feat, stacking_model, kf, num_classes)\n",
    "        removal_results[m] = mean_qwk\n",
    "        print(f\"  Removing {m:12} → mean QWK = {mean_qwk:.4f}\")\n",
    "\n",
    "    # find best improvement\n",
    "    worst_model, candidate_qwk = max(removal_results.items(), key=lambda kv: kv[1])\n",
    "    if candidate_qwk > best_mean:\n",
    "        print(f\"\\n✔ Eliminating model '{worst_model}' improved QWK: {best_mean:.4f} → {candidate_qwk:.4f}\\n\")\n",
    "        # update state\n",
    "        best_mean = candidate_qwk\n",
    "        current_models.remove(worst_model)\n",
    "        current_features = [c for c in current_features if not c.startswith(worst_model + '_')]\n",
    "    else:\n",
    "        print(\"\\n— No single-model removal improved QWK. Elimination complete.\\n\")\n",
    "        break\n",
    "\n",
    "# --- Final training on selected features\n",
    "print(\"▶ Training final 5-fold models on features from:\", current_models)\n",
    "final_mean, final_models = cv_score(current_features, stacking_model, kf, num_classes)\n",
    "print(f\"Final mean QWK: {final_mean:.4f}\\n\")\n",
    "\n",
    "# --- Save each model\n",
    "for i, mdl in enumerate(final_models, 1):\n",
    "    path = f\"stack-tabpfn-fold{i}.pkl\"\n",
    "    joblib.dump(mdl, path)\n",
    "    print(f\"✔ Saved fold {i} model to '{path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51fa516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "class PyTorchTabularMLPRegressor:\n",
    "    \"\"\"A sklearn-like wrapper around a tiny PyTorch MLP with early stopping on QWK.\"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_dims=[128,64],\n",
    "                 dropout=0.3,\n",
    "                 lr=1e-3,\n",
    "                 batch_size=64,\n",
    "                 max_epochs=50,\n",
    "                 patience=5,\n",
    "                 num_classes=5,\n",
    "                 random_state=None,\n",
    "                 device=None):\n",
    "        self.hidden_dims  = hidden_dims\n",
    "        self.dropout      = dropout\n",
    "        self.lr           = lr\n",
    "        self.batch_size   = batch_size\n",
    "        self.max_epochs   = max_epochs\n",
    "        self.patience     = patience\n",
    "        self.num_classes  = num_classes\n",
    "        self.rs           = random_state\n",
    "        self.device       = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self._is_fitted   = False\n",
    "\n",
    "    def _build_model(self, input_dim):\n",
    "        layers = []\n",
    "        dims = [input_dim] + self.hidden_dims\n",
    "        for i in range(len(dims)-1):\n",
    "            layers += [\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.dropout)\n",
    "            ]\n",
    "        layers += [nn.Linear(dims[-1], 1)]\n",
    "        return nn.Sequential(*layers).to(self.device)\n",
    "\n",
    "    def fit(self, X, y, eval_set):\n",
    "        # 1) train/val split for early stopping\n",
    "        X_tr, y_tr = X, y\n",
    "        X_va, y_va = eval_set[0]\n",
    "\n",
    "        # 2) DataLoaders\n",
    "        ds_tr = TensorDataset(torch.from_numpy(X_tr).float(),\n",
    "                              torch.from_numpy(y_tr).float())\n",
    "        ds_va = TensorDataset(torch.from_numpy(X_va).float(),\n",
    "                              torch.from_numpy(y_va).float())\n",
    "        loader_tr = DataLoader(ds_tr, batch_size=self.batch_size,\n",
    "                               shuffle=True,  pin_memory=True)\n",
    "        loader_va = DataLoader(ds_va, batch_size=self.batch_size,\n",
    "                               shuffle=False, pin_memory=True)\n",
    "\n",
    "        # 3) Model, optimizer\n",
    "        model     = self._build_model(X.shape[1])\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr)\n",
    "\n",
    "        best_qwk = -np.inf\n",
    "        best_state = None\n",
    "        no_improve = 0\n",
    "\n",
    "        # 4) Training loop with early stop on QWK\n",
    "        for epoch in range(1, self.max_epochs+1):\n",
    "            model.train()\n",
    "            for xb, yb in loader_tr:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                pred = model(xb).squeeze(1)\n",
    "                loss = nn.functional.mse_loss(pred, yb)\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "            # validation pass\n",
    "            model.eval()\n",
    "            all_p, all_t = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in loader_va:\n",
    "                    xb = xb.to(self.device)\n",
    "                    out = model(xb).cpu().numpy()\n",
    "                    preds = np.clip(np.rint(out), 0, self.num_classes-1).astype(int)\n",
    "                    all_p.append(preds); all_t.append(yb.numpy().astype(int))\n",
    "            all_p = np.concatenate(all_p); all_t = np.concatenate(all_t)\n",
    "            qwk = cohen_kappa_score(all_t, all_p, weights='quadratic')\n",
    "\n",
    "            if qwk > best_qwk:\n",
    "                best_qwk, best_state, no_improve = qwk, model.state_dict(), 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= self.patience:\n",
    "                    break\n",
    "\n",
    "        # load best\n",
    "        model.load_state_dict(best_state)\n",
    "        self.model_ = model\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self._is_fitted:\n",
    "            raise RuntimeError(\"You must fit() before predict().\")\n",
    "        self.model_.eval()\n",
    "        preds = []\n",
    "        loader = DataLoader(torch.from_numpy(X).float(),\n",
    "                            batch_size=self.batch_size,\n",
    "                            shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            for xb in loader:\n",
    "                xb = xb.to(self.device)\n",
    "                out = self.model_(xb).cpu().numpy()\n",
    "                preds.append(out)\n",
    "        return np.concatenate(preds)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # For stacking we want the raw real‐valued preds\n",
    "        return self.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6c880de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Running baseline 5-fold CV on all features …\n",
      "Baseline mean QWK: 0.8416\n",
      "\n",
      "▶ Testing removal of each model …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46129abc5a4f4f3ebc6f32b1fd266aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removing convnext     → mean QWK = 0.8379\n",
      "  Removing convnext-r   → mean QWK = 0.8336\n",
      "  Removing effnet       → mean QWK = 0.8385\n",
      "  Removing effnet-o     → mean QWK = 0.8386\n",
      "  Removing effnet-p     → mean QWK = 0.8328\n",
      "  Removing resnet       → mean QWK = 0.8358\n",
      "  Removing resnet-n     → mean QWK = 0.8333\n",
      "  Removing resnet-r     → mean QWK = 0.8368\n",
      "\n",
      "— No single-model removal improved QWK. Elimination complete.\n",
      "\n",
      "▶ Training final 5-fold models on features from: ['convnext', 'convnext-r', 'effnet', 'effnet-o', 'effnet-p', 'resnet', 'resnet-n', 'resnet-r']\n",
      "Final mean QWK: 0.8353\n",
      "\n",
      "✔ Saved fold 1 model to 'stack-mlp-fold1.pkl'\n",
      "✔ Saved fold 2 model to 'stack-mlp-fold2.pkl'\n",
      "✔ Saved fold 3 model to 'stack-mlp-fold3.pkl'\n",
      "✔ Saved fold 4 model to 'stack-mlp-fold4.pkl'\n",
      "✔ Saved fold 5 model to 'stack-mlp-fold5.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Identify models by splitting on '_'\n",
    "feature_cols = [c for c in df_stack.columns if c != 'label']\n",
    "models = sorted({c.split('_')[0] for c in feature_cols})\n",
    "\n",
    "# Map each model name → list of its feature columns\n",
    "features_by_model = {\n",
    "    m: [c for c in feature_cols if c.startswith(m + '_')]\n",
    "    for m in models\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "stacking_model = PyTorchTabularMLPRegressor(\n",
    "    hidden_dims=[128,64],\n",
    "    dropout=0.4,\n",
    "    lr=1e-3,\n",
    "    batch_size=512,\n",
    "    max_epochs=100,\n",
    "    patience=15,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# --- Baseline\n",
    "print(\"▶ Running baseline 5-fold CV on all features …\")\n",
    "base_mean, base_models = cv_score(feature_cols, stacking_model, kf, num_classes)\n",
    "print(f\"Baseline mean QWK: {base_mean:.4f}\\n\")\n",
    "\n",
    "current_models = models.copy()\n",
    "current_features = feature_cols.copy()\n",
    "best_mean = base_mean\n",
    "\n",
    "# --- Backward elimination\n",
    "while True:\n",
    "    print(\"▶ Testing removal of each model …\")\n",
    "    removal_results = {}\n",
    "    \n",
    "    for m in tqdm(current_models, desc=\"Models\"):\n",
    "        # drop m’s features\n",
    "        feat = [c for c in current_features if not c.startswith(m + '_')]\n",
    "        mean_qwk, _ = cv_score(feat, stacking_model, kf, num_classes)\n",
    "        removal_results[m] = mean_qwk\n",
    "        print(f\"  Removing {m:12} → mean QWK = {mean_qwk:.4f}\")\n",
    "\n",
    "    # find best improvement\n",
    "    worst_model, candidate_qwk = max(removal_results.items(), key=lambda kv: kv[1])\n",
    "    if candidate_qwk > best_mean:\n",
    "        print(f\"\\n✔ Eliminating model '{worst_model}' improved QWK: {best_mean:.4f} → {candidate_qwk:.4f}\\n\")\n",
    "        # update state\n",
    "        best_mean = candidate_qwk\n",
    "        current_models.remove(worst_model)\n",
    "        current_features = [c for c in current_features if not c.startswith(worst_model + '_')]\n",
    "    else:\n",
    "        print(\"\\n— No single-model removal improved QWK. Elimination complete.\\n\")\n",
    "        break\n",
    "\n",
    "# --- Final training on selected features\n",
    "print(\"▶ Training final 5-fold models on features from:\", current_models)\n",
    "final_mean, final_models = cv_score(current_features, stacking_model, kf, num_classes)\n",
    "print(f\"Final mean QWK: {final_mean:.4f}\\n\")\n",
    "\n",
    "# --- Save each model\n",
    "for i, mdl in enumerate(final_models, 1):\n",
    "    path = f\"stack-mlp-fold{i}.pkl\"\n",
    "    joblib.dump(mdl, path)\n",
    "    print(f\"✔ Saved fold {i} model to '{path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1a97b",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convnext_0',\n",
       " 'convnext_1',\n",
       " 'convnext_2',\n",
       " 'convnext_3',\n",
       " 'convnext_4',\n",
       " 'convnext-r_pred',\n",
       " 'effnet_0',\n",
       " 'effnet_1',\n",
       " 'effnet_2',\n",
       " 'effnet_3',\n",
       " 'effnet_4',\n",
       " 'effnet-p_0',\n",
       " 'effnet-p_1',\n",
       " 'effnet-p_2',\n",
       " 'effnet-p_3',\n",
       " 'effnet-p_4',\n",
       " 'effnet-o_0',\n",
       " 'effnet-o_1',\n",
       " 'effnet-o_2',\n",
       " 'effnet-o_3',\n",
       " 'resnet_0',\n",
       " 'resnet_1',\n",
       " 'resnet_2',\n",
       " 'resnet_3',\n",
       " 'resnet_4',\n",
       " 'resnet-n_0',\n",
       " 'resnet-n_1',\n",
       " 'resnet-n_2',\n",
       " 'resnet-n_3',\n",
       " 'resnet-n_4',\n",
       " 'resnet-r_pred']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [c for c in df_stack.columns if c != 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bf21ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['convnext_0',\n",
    "                'convnext_1',\n",
    "                'convnext_2',\n",
    "                'convnext_3',\n",
    "                'convnext_4',\n",
    "                'convnext-r_pred',\n",
    "                'effnet_0',\n",
    "                'effnet_1',\n",
    "                'effnet_2',\n",
    "                'effnet_3',\n",
    "                'effnet_4',\n",
    "                'effnet-p_0',\n",
    "                'effnet-p_1',\n",
    "                'effnet-p_2',\n",
    "                'effnet-p_3',\n",
    "                'effnet-p_4',\n",
    "                'effnet-o_0',\n",
    "                'effnet-o_1',\n",
    "                'effnet-o_2',\n",
    "                'effnet-o_3']\n",
    "                # 'resnet_0',\n",
    "                # 'resnet_1',\n",
    "                # 'resnet_2',\n",
    "                # 'resnet_3',\n",
    "                # 'resnet_4',\n",
    "                # 'resnet-n_0',\n",
    "                # 'resnet-n_1',\n",
    "                # 'resnet-n_2',\n",
    "                # 'resnet-n_3',\n",
    "                # 'resnet-n_4',\n",
    "                # 'resnet-r_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76d1e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mean QWK: 0.8389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "stacking_model = LGBMRegressor(verbose=-1)\n",
    "\n",
    "final_mean, final_models = cv_score(feature_cols, stacking_model, kf, num_classes)\n",
    "print(f\"Final mean QWK: {final_mean:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02f910f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mean QWK: 0.8398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "stacking_model = LGBMRegressor(verbose=-1, device='cuda')\n",
    "\n",
    "final_mean, final_models = cv_score(feature_cols, stacking_model, kf, num_classes)\n",
    "print(f\"Final mean QWK: {final_mean:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f1a5c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mean QWK: 0.8437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "stacking_model = CatBoostRegressor(verbose=0, task_type='GPU')\n",
    "\n",
    "final_mean, final_models = cv_score(feature_cols, stacking_model, kf, num_classes)\n",
    "print(f\"Final mean QWK: {final_mean:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b37ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-dl-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
