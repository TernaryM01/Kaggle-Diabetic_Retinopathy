{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec70bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, labels_df, img_dir, ext, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels_df (pd.DataFrame): DataFrame with columns ['id_code', 'diagnosis']\n",
    "            img_dir (Path): Directory where images are stored\n",
    "            transform (callable, optional): Transform to apply to images\n",
    "        \"\"\"\n",
    "        self.labels_df = labels_df\n",
    "        self.img_dir = img_dir\n",
    "        self.ext = ext\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        img_id = row.iloc[0]  # First column: image filename base\n",
    "        label = int(row.iloc[1])  # Second column: label\n",
    "\n",
    "        img_path = self.img_dir / f\"{img_id}.{self.ext}\"\n",
    "        image = Image.open(img_path).convert('RGB')  # ensure 3 channels\n",
    "\n",
    "        if self.transform:\n",
    "            image = np.array(image)\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcb68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "IMAGE_SIZE_TRAIN = 352\n",
    "IMAGE_SIZE_VAL = 480\n",
    "\n",
    "trainval_labels1 = pd.read_csv('aptos2019-blindness-detection/train.csv')\n",
    "trainval_imgs_dir1 = Path('aptos2019-blindness-detection/train_images/processed')\n",
    "\n",
    "trainval_labels2 = pd.read_csv('diabetic-retinopathy-detection/trainLabelsHalf.csv')\n",
    "trainval_imgs_dir2 = Path('diabetic-retinopathy-detection/train/processed')\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE_VAL, IMAGE_SIZE_VAL),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=360, p=1.0),\n",
    "    A.RandomCrop(IMAGE_SIZE_TRAIN, IMAGE_SIZE_TRAIN),\n",
    "\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=0.2,  # ±20% brightness\n",
    "        contrast_limit=0.2,    # ±20% contrast\n",
    "        p=0.3\n",
    "    ),\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=10,        # ±10 degrees\n",
    "        sat_shift_limit=20,        # ±20%\n",
    "        val_shift_limit=10,        # ±10%\n",
    "        p=0.3\n",
    "    ),\n",
    "    A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n",
    "    A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.3),\n",
    "\n",
    "    A.Normalize(  # For model pretrained on ImageNet\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE_VAL, IMAGE_SIZE_VAL),\n",
    "    A.Normalize(  # For model pretrained on ImageNet\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a8252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# First, create your untransformed base datasets\n",
    "base_dataset1 = TrainDataset(trainval_labels1, trainval_imgs_dir1, 'png')\n",
    "base_dataset2 = TrainDataset(trainval_labels2, trainval_imgs_dir2, 'jpeg')\n",
    "\n",
    "# Concatenate just to get total size and allow for consistent indexing\n",
    "base_dataset = ConcatDataset([base_dataset1, base_dataset2])\n",
    "\n",
    "total_size = len(base_dataset)\n",
    "\n",
    "del base_dataset1, base_dataset2, base_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eafdaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, Subset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Split indices (use seed for reproducibility)\n",
    "indices = np.arange(total_size)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split indices into train/val\n",
    "train_size = int(0.8 * total_size)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "# Rebuild the original datasets with their respective transforms\n",
    "train_dataset1 = TrainDataset(trainval_labels1, trainval_imgs_dir1, 'png', train_transform)\n",
    "train_dataset2 = TrainDataset(trainval_labels2, trainval_imgs_dir2, 'jpeg', train_transform)\n",
    "val_dataset1   = TrainDataset(trainval_labels1, trainval_imgs_dir1, 'png', val_transform)\n",
    "val_dataset2   = TrainDataset(trainval_labels2, trainval_imgs_dir2, 'jpeg', val_transform)\n",
    "\n",
    "# Concat them separately\n",
    "train_dataset_full = ConcatDataset([train_dataset1, train_dataset2])\n",
    "val_dataset_full   = ConcatDataset([val_dataset1, val_dataset2])\n",
    "\n",
    "# Subset them using the indices\n",
    "train_dataset = Subset(train_dataset_full, train_indices)\n",
    "val_dataset   = Subset(val_dataset_full, val_indices)\n",
    "\n",
    "BATCH_SIZE_TRAIN = 12\n",
    "BATCH_SIZE_VAL = 10\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True, num_workers=10)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d2c7c",
   "metadata": {},
   "source": [
    "==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c14c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c00a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"efficientnet_b3\", lr=1e-3, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # create & swap in a new head\n",
    "        self.net = timm.create_model(\n",
    "            self.hparams.model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=self.hparams.num_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        acc  = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc',  acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        acc  = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc',  acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',              # we're watching val_loss (lower is better)\n",
    "                factor=0.5,              # reduce LR by this factor\n",
    "                patience=5,              # after N epochs of no improvement\n",
    "                min_lr=1e-6,             # don’t go below this\n",
    "                verbose=True\n",
    "            ),\n",
    "            'monitor': 'val_loss',      # <- this is important\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaac5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.seed_everything(42)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='effnet-b3-{epoch:02d}-{val_loss:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=[earlystop_cb, checkpoint_cb],\n",
    "    accelerator='auto',  # GPU if available\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "model = EfficientNetClassifier(lr=1e-3, num_classes=5)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "print(\"✅ Best checkpoint:\", checkpoint_cb.best_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d7120",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d550be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetFromScratchClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"efficientnet_b3\", lr=1e-3, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # create & swap in a new head\n",
    "        self.net = timm.create_model(\n",
    "            self.hparams.model_name,\n",
    "            pretrained=False,\n",
    "            num_classes=self.hparams.num_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        acc  = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc',  acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        acc  = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc',  acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',              # we're watching val_loss (lower is better)\n",
    "                factor=0.5,              # reduce LR by this factor\n",
    "                patience=5,              # after N epochs of no improvement\n",
    "                min_lr=1e-6,             # don’t go below this\n",
    "                verbose=True\n",
    "            ),\n",
    "            'monitor': 'val_loss',      # <- this is important\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a692ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.seed_everything(42)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='effnet-b3-scratch-{epoch:02d}-{val_loss:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=[earlystop_cb, checkpoint_cb],\n",
    "    accelerator='auto',  # GPU if available\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "model = EfficientNetFromScratchClassifier(lr=1e-3, num_classes=5)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "print(\"✅ Best checkpoint:\", checkpoint_cb.best_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10424eb8",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559c87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AdjacentLabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1, num_classes=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            smoothing: Total smoothing weight to be distributed to adjacent classes.\n",
    "            num_classes: Total number of classes (assumed to be 0-indexed and ordinal).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logits)\n",
    "            for i in range(logits.size(0)):\n",
    "                t = target[i]\n",
    "                if t > 0 and t < self.num_classes - 1:\n",
    "                    true_dist[i][t]     = 1.0 - self.smoothing\n",
    "                    true_dist[i][t - 1] = self.smoothing / 2\n",
    "                    true_dist[i][t + 1] = self.smoothing / 2\n",
    "                elif t == 0:\n",
    "                    true_dist[i][t]     = 1.0 - self.smoothing\n",
    "                    true_dist[i][t + 1] = self.smoothing\n",
    "                elif t == self.num_classes - 1:\n",
    "                    true_dist[i][t]     = 1.0 - self.smoothing\n",
    "                    true_dist[i][t - 1] = self.smoothing\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        return -(true_dist * log_probs).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c01cdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2Classifier(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"efficientnetv2_rw_m\", lr=1e-4, num_classes=5, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # create & swap in a new head\n",
    "        self.net = timm.create_model(\n",
    "            self.hparams.model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=self.hparams.num_classes,\n",
    "            \n",
    "            drop_rate=0.4,        # 🔥 add stronger dropout (applied before final FC)\n",
    "            drop_path_rate=0.3,   # 🔥 stochastic depth (helps regularize deep nets)\n",
    "        )\n",
    "        \n",
    "        self.criterion = AdjacentLabelSmoothingLoss(\n",
    "            smoothing=self.hparams.smoothing,\n",
    "            num_classes=self.hparams.num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        acc  = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc',  acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        acc  = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc',  acc, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=1e-4)\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',              # we're watching val_loss (lower is better)\n",
    "                factor=0.5,              # reduce LR by this factor\n",
    "                patience=5,              # after N epochs of no improvement\n",
    "                min_lr=1e-6,             # don’t go below this\n",
    "                verbose=True\n",
    "            ),\n",
    "            'monitor': 'val_loss',\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c4fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/mauribuntu/miniconda3/envs/causal-dl-torch/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /mnt/g/Kaggle-Diabetic-Retinopathy/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                       | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | net       | EfficientNet               | 51.1 M | train\n",
      "1 | criterion | AdjacentLabelSmoothingLoss | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "51.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "51.1 M    Total params\n",
      "204.377   Total estimated model params size (MB)\n",
      "1117      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab06437ec0e84eadb989bdca7e18281e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122d0b373a9b4bc6916faadb390fe565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4bf3374079491fbc94c5d0bec1bec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419e35f91da14d37b27c38326f4fe3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca0caf456354418800f8a28d35a5949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c5a2778a1f4334a9a0a5bf7db13337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d0cbab072240fb99da67aabc3669d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdef9fe03951464ba8c02fa5f3f04b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde8f224fda74d1cb87a33ccaf1f9e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4513c709e50452286e162497aaedce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460fdb84e747405abff3fba1b480d97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pl.seed_everything(42)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='effnet-v2rw-m-dropout2-l2reg2-augs-adjsmooth-{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=[earlystop_cb, checkpoint_cb],\n",
    "    accelerator='auto',  # GPU if available\n",
    "    precision='16-mixed',\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "model = EfficientNetV2Classifier(lr=1e-4, num_classes=5)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "print(\"✅ Best checkpoint:\", checkpoint_cb.best_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7b3fb",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd15892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EfficientNetV2OrdinalClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"efficientnetv2_rw_m\", lr=1e-4, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Use timm to load pretrained backbone, remove classifier head\n",
    "        self.net = timm.create_model(\n",
    "            self.hparams.model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # remove original head\n",
    "            \n",
    "            drop_rate=0.4,\n",
    "            drop_path_rate=0.3\n",
    "        )\n",
    "\n",
    "        in_features = self.net.num_features\n",
    "        self.head = nn.Linear(in_features, self.num_classes - 1)  # 4 outputs for 5 ordinal classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.net(x)\n",
    "        logits = self.head(features)\n",
    "        return logits\n",
    "    \n",
    "    def ordinal_targets(self, labels):\n",
    "        \"\"\"\n",
    "        Converts integer class labels (0 to num_classes - 1) into ordinal binary targets.\n",
    "        For example, label 2 becomes [1, 1, 0, 0] for num_classes = 5\n",
    "        \"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        num_thresholds = self.num_classes - 1  # one less than number of classes\n",
    "        labels_expanded = labels.unsqueeze(1)  # Expand labels to shape (batch_size, 1)\n",
    "        # Create comparison thresholds: shape (1, num_thresholds) = [0, 1, 2, 3]\n",
    "        thresholds = torch.arange(num_thresholds, device=labels.device).unsqueeze(0)\n",
    "        # Compare each label to thresholds: True where label > threshold\n",
    "        binary_targets = labels_expanded > thresholds  # shape (batch_size, num_thresholds)\n",
    "        return binary_targets.float()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        targets = self.ordinal_targets(labels)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "        \n",
    "        preds = (logits.sigmoid() > 0.5).sum(dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc',  acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        logits = self(imgs)\n",
    "        targets = self.ordinal_targets(labels)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "\n",
    "        preds = torch.clamp((logits.sigmoid() > 0.5).sum(dim=1), 0, self.num_classes - 1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc',  acc, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=1e-4)\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',              # we're watching val_loss (lower is better)\n",
    "                factor=0.5,              # reduce LR by this factor\n",
    "                patience=5,              # after N epochs of no improvement\n",
    "                min_lr=1e-6,             # don’t go below this\n",
    "                verbose=True\n",
    "            ),\n",
    "            'monitor': 'val_loss',\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a9a0c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mautolog()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# pl.seed_everything(42)\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlflow'"
     ]
    }
   ],
   "source": [
    "import mlflow.pytorch\n",
    "\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='effnet-v2rw-m-ordinal-dropout-l2reg-augs-{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    callbacks=[earlystop_cb, checkpoint_cb],\n",
    "    accelerator='auto',  # GPU if available\n",
    "    precision='16-mixed',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "model = EfficientNetV2OrdinalClassifier(lr=1e-4, num_classes=5)\n",
    "trainer.fit(model, train_dataloader, val_dataloader,\n",
    "            ckpt_path='checkpoints/effnet-v2rw-m-ordinal-dropout-l2reg-augs-epoch=04-val_loss=0.1859-val_acc=0.8198.ckpt')\n",
    "\n",
    "print(\"✅ Best checkpoint:\", checkpoint_cb.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed88f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-dl-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
